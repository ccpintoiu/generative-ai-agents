{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfydLmn6YNfv"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#Author: Victor Dantas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqicuhPWatlJ"
   },
   "source": [
    "# Generate a fine-tuning dataset for code-bison model from 100 examples\n",
    "This Notebook helps automate and simplify the task of generating code fine-tuning dataset, starting from a Google Sheet containing 100 examples of prompts and corresponding generated code (no special formatting required, just copied and pasted from e.g. an IDE).\n",
    "\n",
    "In this example, the following dataset is used:\n",
    "\n",
    "[Code generation fine-tuning dataset (Python)](https://docs.google.com/spreadsheets/d/1CU5SSf6tVLqXtYUE8F5rFmJyzoZ6gx7YCuSyyZpHRMg/edit?usp=sharing&resourcekey=0-SjhKOlrgOOMvGqPvEAZ0aA)\n",
    "\n",
    "This is a synthetic dataset of Python code tasks where the code output always begins with the disclaimer comment \"The following code was generated by AI\". The code tasks span data structure manipulation, file manipulation, statistics, pandas, and use of Google Cloud Python client libraries. Essentially, we're trying to fine-tune the model to always add a comment to generated code saying that the code was AI-generated.\n",
    "\n",
    "This Notebook will:\n",
    "\n",
    "1. Augment the dataset by using PaLM LLM to generate 4 additional prompt variants for each prompt in the dataset. This is to help capture more ways people could ask the same question.\n",
    "2. Convert the augmented dataset to a JSONL file with code markdown formatting applied.\n",
    "3. Upload the JSONL file to a GCS bucket for use in a fine-tuning pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm_BcmfZA-Ff"
   },
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohPUPez8imvE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install google-cloud-aiplatform langchain  --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3fd2805"
   },
   "source": [
    "**Colab only:** Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c49ae51e03fe",
    "outputId": "635497c9-b7b5-4036-e6c2-ddb0c1cf6842",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "034fe628"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "**Colab only:** If you are using Colab to run this notebook, uncomment the cell below and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d83f50a3d3f"
   },
   "outputs": [],
   "source": [
    "#from google.colab import auth as google_auth\n",
    "#google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a92ac8ea"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRyKKIqubWsi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"PROJECT_ID\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvJOaR7KbPBK"
   },
   "source": [
    "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD8T5hkzbOhv"
   },
   "outputs": [],
   "source": [
    "## Vertex AI\n",
    "# import vertexai\n",
    "\n",
    "#vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e62a5503",
    "outputId": "609ca1ce-1366-4e82-e73e-f849f2c7910a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed40bac2"
   },
   "source": [
    "### Using Langchain and designing a prompt for dataset augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aaf36cd9",
    "outputId": "71754205-9e1f-4272-8ffe-f1ff476fa8b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@latest\", #\"text-bison-32@latest\",\n",
    "    max_output_tokens=2048,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template='''Given a prompt, generate 4 additional ways of asking the exact same thing by rephrashing the prompt slightly each time.\n",
    "    These are intended as prompts to a code generation language model and should preferrably be short and concise, not formal, and\n",
    "    should simulate the way developers may prompt a code model to help them with a code task.\n",
    "    Have one of the prompts not include any instruction word (such as write, generate, implement, etc.) but simply a short description\n",
    "    of the task at hand (for example: Python function that reverses a string).\n",
    "    Generate the response as one answer per line.\n",
    "    \\n Prompt: {question}''',\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(question=\"Write a Python function that takes a list of numbers and returns the average\")\n",
    "llm(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NHB5c-FL6Do"
   },
   "source": [
    "### Augmenting the base fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abU38e5UCVqf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"vertex-\" + UUID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "REGION = LOCATION\n",
    "\n",
    "# Create a bucket\n",
    "! gsutil mb -l $REGION $BUCKET_URI\n",
    "\n",
    "# Relative path to base dataset file (export from Sheet)\n",
    "DATASET = 'finetuning_dataset.csv'  # @param {type:\"string\"}\n",
    "! gsutil cp $DATASET $BUCKET_NAME\n",
    "\n",
    "DATASET_GCS_URI = f\"{BUCKET_URI}/{DATASET}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9qU69jJKT4Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import date\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# Helper function for repeated experimentation\n",
    "def augment_dataset(csv_filename, num_of_rows = None):\n",
    "  line_num = 0\n",
    "  skip_count = 0\n",
    "  results = []\n",
    "  with open(csv_filename) as csv_file:\n",
    "      try:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "          line_num += 1\n",
    "          if line_num == 1: # skip header row\n",
    "            continue\n",
    "\n",
    "          original_prompt = row[0].strip()\n",
    "          code_output = row[1]\n",
    "\n",
    "          try:\n",
    "            llm_output = llm(prompt_template.format(question=original_prompt))\n",
    "            llm_output = llm_output.replace('1. ','').replace('2. ','').replace('3. ','').replace('4. ','').strip()\n",
    "            alternative_prompts = llm_output.split('\\n')\n",
    "            if len(alternative_prompts) != 4:\n",
    "              raise Exception(f\"Expected 4 alternative prompts, but got {len(alternative_prompts)}\")\n",
    "\n",
    "            results.append({'prompt': original_prompt, 'code_output': code_output})\n",
    "            for prompt in alternative_prompts:\n",
    "              #print(prompt)\n",
    "              results.append({'prompt': prompt, 'code_output': code_output})\n",
    "          except Exception as e:\n",
    "            print(\">>> WARNING:\", e)\n",
    "            skip_count += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "          if line_num == num_of_rows:\n",
    "            break\n",
    "\n",
    "          #endfor\n",
    "      except Exception as e:\n",
    "        print(f\"ERROR: Something went wrong:\", e)\n",
    "\n",
    "      finally:\n",
    "        print(f\"\\nProcesed {line_num-1} prompts. Skipped {skip_count}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def write_results_to_csv(results, filename = 'results.csv'):\n",
    "  keys = results[0].keys()\n",
    "\n",
    "  with open(filename, 'w', newline='') as output_file:\n",
    "      dict_writer = csv.DictWriter(output_file, keys)\n",
    "      dict_writer.writeheader()\n",
    "      dict_writer.writerows(results)\n",
    "\n",
    "\n",
    "def format_and_convert_to_jsonl(input_csv_filename, output_jsonl_filename):\n",
    "  line_num = 0\n",
    "  dictl = []\n",
    "  with open(input_csv_filename) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        dict_entry = {}\n",
    "        line_num += 1\n",
    "        if line_num == 1: # skip header row\n",
    "            continue\n",
    "\n",
    "        prompt = row[0]\n",
    "        code = row[1]\n",
    "        #raw_code = code.encode('unicode-escape').decode()\n",
    "        formatted_code = f'```python\\n{code}\\n```'\n",
    "\n",
    "        dict_entry['input_text'] = prompt\n",
    "        dict_entry['output_text'] = formatted_code\n",
    "\n",
    "        dictl.append(dict_entry)\n",
    "        # TODO: add prompt variations and append\n",
    "\n",
    "  with open(output_jsonl_filename, 'a') as jsonl_file:\n",
    "      for line in dictl:\n",
    "          jsonl_file.write(json.dumps(line))\n",
    "          jsonl_file.write('\\n')\n",
    "\n",
    "\n",
    "def upload_file_to_gcs(filename, bucket_name = BUCKET_NAME):\n",
    "  today_date = date.today().strftime('%Y%m%d')\n",
    "  storage_client = storage.Client()\n",
    "  bucket = storage_client.bucket(bucket_name)\n",
    "  blob = bucket.blob(f\"{today_date}_{filename}\")\n",
    "  blob.upload_from_filename(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lztSY0zsCmKC",
    "outputId": "e7e0df0c-4547-41da-8d2b-4238748eaa24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set num_of_rows to test on a small subset of rows. If set to None, it will read the entire file\n",
    "results = augment_dataset(DATASET, num_of_rows = 3)\n",
    "# results = augment_dataset(DATASET, num_of_rows = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUMlsfI4U-wn"
   },
   "source": [
    "### Exporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmgdqbbNDPyC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_results_to_csv(results, filename = 'augmented_fine_tuning_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWtf1zUqnp9U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "format_and_convert_to_jsonl('augmented_fine_tuning_dataset.csv', 'augmented_fine_tuning_dataset.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NroU5jNbNnnQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "upload_file_to_gcs('augmented_fine_tuning_dataset.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Here are some recommended configurations for tuning a code foundation model. You can find more in the [documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-code-models#recommended-configurations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "MODEL_NAME = f\"genai-workshop-tuned-model-{UUID}\"\n",
    "\n",
    "TRAINING_STEPS = 200\n",
    "\n",
    "pipeline_arguments = {\n",
    "    \"model_display_name\": MODEL_NAME,\n",
    "    \"location\": REGION,\n",
    "    \"large_model_reference\": \"code-bison@latest\",\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"train_steps\": TRAINING_STEPS,\n",
    "    \"dataset_uri\": DATASET_GCS_URI,\n",
    "}\n",
    "\n",
    "pipeline_root = f'{BUCKET_URI}/{MODEL_NAME}'\n",
    "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0'\n",
    "\n",
    "# Function that starts the tuning job\n",
    "def tuned_model(\n",
    "    location: str,\n",
    "    template_path: str,\n",
    "    model_display_name: str,\n",
    "    pipeline_arguments: str,\n",
    "):\n",
    "    \"\"\"Prompt-tune a new model, based on a prompt-response data.\n",
    "\n",
    "    \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
    "    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
    "    DataFrame. Each training example should be JSONL record with two keys, for\n",
    "    example:\n",
    "      {\n",
    "        \"input_text\": <input prompt>,\n",
    "        \"output_text\": <associated output>\n",
    "      },\n",
    "\n",
    "    Args:\n",
    "      project_id: GCP Project ID, used to initialize aiplatform\n",
    "      location: GCP Region, used to initialize aiplatform\n",
    "      template_path: path to the template\n",
    "      model_display_name: Name for your model.\n",
    "      pipeline_arguments: arguments used during pipeline runtime\n",
    "    \"\"\"\n",
    "\n",
    "    aiplatform.init(project=PROJECT_ID, location=location)\n",
    "\n",
    "    \n",
    "\n",
    "    job = aiplatform.PipelineJob(\n",
    "        template_path=template_path,\n",
    "        display_name=model_display_name,\n",
    "        parameter_values=pipeline_arguments,\n",
    "        location=REGION,\n",
    "        pipeline_root=pipeline_root,\n",
    "        enable_caching=True,\n",
    "    )\n",
    "\n",
    "    return job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = tuned_model(REGION, template_path, MODEL_NAME, pipeline_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
